{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "# import pandas_profiling\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, confusion_matrix, recall_score, precision_score, f1_score,mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn import metrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', 250)\n",
    "pd.set_option('display.max_rows', 250)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import  Pool\n",
    "def parallelize_dataframe(df, func, n_cores=12):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    mape = (np.abs((y_true - y_pred)/(y_true))).mean()\n",
    "    return mape \n",
    "\n",
    "def preprocessing(train,test):\n",
    "    cat_cols = train.select_dtypes(\"object\").columns\n",
    "    for col in tqdm(cat_cols):\n",
    "        if col in train.columns:\n",
    "            train[col] = train[col].fillna(\"unseen\").astype(str)\n",
    "            test[col] = test[col].fillna(\"unseen\").astype(str)\n",
    "            le = LabelEncoder()\n",
    "            le.fit(list(train[col])+list(test[col]))\n",
    "            train[col] = le.transform(train[col])\n",
    "            train[col] = train[col].astype(\"category\")        \n",
    "            test[col] = le.transform(test[col])\n",
    "            test[col] = test[col].astype(\"category\")         \n",
    "    return train, test\n",
    "\n",
    "def kfold_lightgbm(params, train_df, test_df, FEATS_EXCLUDED,n_estimators, LOCAL_TEST=True):\n",
    "    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n",
    "    num_folds = 3\n",
    "    folds = KFold(n_splits = num_folds, shuffle=True, random_state=326)\n",
    "    models = []\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance = pd.DataFrame()\n",
    "    final_rmse = 0\n",
    "    final_mape = 0\n",
    "    final_mae = 0\n",
    "    feats = [f for f in train_df.columns if f not in FEATS_EXCLUDED+[\"PSGR_COUNT\"]]\n",
    "    clfs = []\n",
    "    # k-fold\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['PSGR_COUNT'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['PSGR_COUNT'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['PSGR_COUNT'].iloc[valid_idx]\n",
    "    \n",
    "        lgb_train = lgb.Dataset(train_x,label=train_y,free_raw_data=False)\n",
    "        lgb_test = lgb.Dataset(valid_x,label=valid_y,free_raw_data=False)\n",
    "\n",
    "        reg = lgb.train(params,lgb_train,valid_sets=[lgb_train, lgb_test],valid_names=['train', 'valid'],\n",
    "                        num_boost_round=n_estimators,early_stopping_rounds= 200,verbose_eval=1000)\n",
    "        \n",
    "        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "\n",
    "        # set data structure\n",
    "        #reg = lgb.LGBMRegressor(**params, n_estimators = n_estimators)   \n",
    "        #reg.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "        #          eval_metric=\"auc\",verbose=333, early_stopping_rounds=333)       \n",
    "        \n",
    "        #oof_preds[valid_idx] = reg.predict(valid_x)\n",
    "        if LOCAL_TEST==False:\n",
    "            sub_preds += reg.predict(test_df[feats], num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "        imp = pd.DataFrame()\n",
    "        imp[\"feature\"] = train_x.columns\n",
    "        imp[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n",
    "        imp[\"fold\"] = n_fold + 1\n",
    "        feature_importance = pd.concat([feature_importance, imp], axis=0)\n",
    "         \n",
    "        curr_rmse = sqrt(mean_squared_error(valid_y, oof_preds[valid_idx]))\n",
    "        curr_mape = mape(valid_y, oof_preds[valid_idx]) \n",
    "        curr_mae = mean_absolute_error(valid_y, oof_preds[valid_idx])\n",
    "        \n",
    "        print('Fold %2d -> rmse : %.6f -- mape : %.6f -- mae : %.6f' % (n_fold + 1, curr_rmse,curr_mape,curr_mae)) \n",
    "        del reg, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "        \n",
    "    final_rmse = sqrt(mean_squared_error(train_df['PSGR_COUNT'], oof_preds))\n",
    "    final_mape = mape(train_df['PSGR_COUNT'], oof_preds) \n",
    "    final_mae = mean_absolute_error(train_df['PSGR_COUNT'], oof_preds)\n",
    "              \n",
    "    print('Overall RMSE : %.6f - Overall MAPE : %.6f - Overall MAE : %.6f' % (final_rmse,final_mape,final_mae))\n",
    "        \n",
    "    return sub_preds, oof_preds, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_features(df):\n",
    "    df['LEG1_DEP_DATE_GMT_MONTH'] = df['LEG1_DEP_DATE_GMT'].dt.month.astype(np.int8)\n",
    "    df['LEG1_DEP_DATE_GMT_WEEK'] = df['LEG1_DEP_DATE_GMT'].dt.weekofyear.astype(np.int8)\n",
    "    df['LEG1_DEP_DATE_GMT_DAYOFYEAR'] = df['LEG1_DEP_DATE_GMT'].dt.dayofyear.astype(np.int16)\n",
    "    df['LEG1_DEP_DATE_GMT_DAYOFWEEK'] = df['LEG1_DEP_DATE_GMT'].dt.dayofweek.astype(np.int16)\n",
    "    df['LEG1_DEP_DATE_GMT_DAYOFMONTH'] = df['LEG1_DEP_DATE_GMT'].dt.day.astype(np.int16)\n",
    "\n",
    "def time_features(df):\n",
    "    df[\"LEG1_DEP_TIME_GMT_HOUR\"] = df[\"LEG1_DEP_TIME_GMT\"].apply(lambda x: int(x)//100)\n",
    "    df[\"LEG1_DEP_TIME_GMT_MINUTE\"] = df[\"LEG1_DEP_TIME_GMT\"].apply(lambda x: int(x)%100)\n",
    "    df[\"LEG1_DEP_DATE_GMT\"] = df[\"LEG1_DEP_DATE_GMT\"].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    df[\"LEG1_DEP_DATE_GMT\"] = pd.to_datetime(df[\"LEG1_DEP_DATE_GMT\"]) + pd.to_timedelta(df[\"LEG1_DEP_TIME_GMT_HOUR\"], unit='h') + pd.to_timedelta(df[\"LEG1_DEP_TIME_GMT_MINUTE\"], unit='m')\n",
    "\n",
    "    df[\"LEG2_DEP_TIME_GMT_HOUR\"] = df[\"LEG2_DEP_TIME_GMT\"].apply(lambda x: int(x)//100)\n",
    "    df[\"LEG2_DEP_TIME_GMT_MINUTE\"] = df[\"LEG2_DEP_TIME_GMT\"].apply(lambda x: int(x)%100)\n",
    "    df[\"LEG2_DEP_DATE_GMT\"] = df[\"LEG2_DEP_DATE_GMT\"].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    df[\"LEG2_DEP_DATE_GMT\"] = pd.to_datetime(df[\"LEG2_DEP_DATE_GMT\"]) + pd.to_timedelta(df[\"LEG2_DEP_TIME_GMT_HOUR\"], unit='h') + pd.to_timedelta(df[\"LEG2_DEP_TIME_GMT_MINUTE\"], unit='m')\n",
    "\n",
    "    df[\"LEG1_ARR_TIME_GMT_HOUR\"] = df[\"LEG1_ARR_TIME_GMT\"].apply(lambda x: int(x)//100)\n",
    "    df[\"LEG1_ARR_TIME_GMT_MINUTE\"] = df[\"LEG1_ARR_TIME_GMT\"].apply(lambda x: int(x)%100)\n",
    "    df[\"LEG1_ARR_DATE_GMT\"] = df[\"LEG1_ARR_DATE_GMT\"].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    df[\"LEG1_ARR_DATE_GMT\"] = pd.to_datetime(df[\"LEG1_ARR_DATE_GMT\"]) + pd.to_timedelta(df[\"LEG1_ARR_TIME_GMT_HOUR\"], unit='h') + pd.to_timedelta(df[\"LEG1_ARR_TIME_GMT_MINUTE\"], unit='m')\n",
    "\n",
    "    df[\"LEG2_ARR_TIME_GMT_HOUR\"] = df[\"LEG2_ARR_TIME_GMT\"].apply(lambda x: int(x)//100)\n",
    "    df[\"LEG2_ARR_TIME_GMT_MINUTE\"] = df[\"LEG2_ARR_TIME_GMT\"].apply(lambda x: int(x)%100)\n",
    "    df[\"LEG2_ARR_DATE_GMT\"] = df[\"LEG2_ARR_DATE_GMT\"].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d'))\n",
    "    df[\"LEG2_ARR_DATE_GMT\"] = pd.to_datetime(df[\"LEG2_ARR_DATE_GMT\"]) + pd.to_timedelta(df[\"LEG2_ARR_TIME_GMT_HOUR\"], unit='h') + pd.to_timedelta(df[\"LEG2_ARR_TIME_GMT_MINUTE\"], unit='m')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'PSGR_COUNT'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['PSGR_COUNT'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'PSGR_COUNT'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['PSGR_COUNT'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_dict ={}\n",
    "for col in ['CARRIER', 'FLIGHT_NO', 'AIRCRAFT_TYPE', 'OND', 'OND_SELL_CLASS','LEG1_SELL_CLASS', 'OND_CABIN_CLASS',\n",
    "            'LEG1_CABIN_CLASS', 'ORIGIN','HUB', 'DESTINATION']:\n",
    "    dtypes_dict[col]=\"category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90562838, 23) (100000, 23)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"datathon_case_2/case_2_train_data.csv\",dtype=dtypes_dict)\n",
    "test = pd.read_csv(\"datathon_case_2/case_2_result.csv\",dtype=dtypes_dict)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parallelize_dataframe(df = train, func = time_features)\n",
    "test = parallelize_dataframe(df = test, func = time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train.pkl')\n",
    "# test.to_pickle('test.pkl')\n",
    "\n",
    "# train = pickle.load( open( \"train.pkl\", \"rb\" ) )\n",
    "# test = pickle.load( open( \"test.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"FLIGHT_NO\"] = train[\"FLIGHT_NO\"].astype(str).fillna(\"UNSEEN\").astype(\"category\")\n",
    "train[\"AIRCRAFT_TYPE\"] = train[\"AIRCRAFT_TYPE\"].astype(str).fillna(\"UNSEEN\").astype(\"category\")\n",
    "test[\"FLIGHT_NO\"] = test[\"FLIGHT_NO\"].astype(str).fillna(\"UNSEEN\").astype(\"category\")\n",
    "test[\"AIRCRAFT_TYPE\"] = test[\"AIRCRAFT_TYPE\"].astype(str).fillna(\"UNSEEN\").astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['CARRIER', 'FLIGHT_NO', 'AIRCRAFT_TYPE', 'OND', 'ORIGIN','HUB', 'DESTINATION']:\n",
    "    trn, sub = target_encode(train[col],test[col], target=train[\"PSGR_COUNT\"], \n",
    "                             min_samples_leaf=100,smoothing=10, noise_level=0.01)\n",
    "    \n",
    "    train[col+\"_MEAN\"] = trn\n",
    "    test[col+\"_MEAN\"] = sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARRIER\n",
      "FLIGHT_NO\n",
      "AIRCRAFT_TYPE\n",
      "OND\n",
      "ORIGIN\n",
      "HUB\n",
      "DESTINATION\n"
     ]
    }
   ],
   "source": [
    "#Frequency of Categorical values\n",
    "for col in ['CARRIER', 'FLIGHT_NO', 'AIRCRAFT_TYPE', 'OND', 'ORIGIN','HUB', 'DESTINATION']:\n",
    "    print(col)\n",
    "    freq_dict = pd.concat([train[col],test[col]]).value_counts().to_dict()\n",
    "    train[col+\"_FREQ\"] = train[col].map(freq_dict)\n",
    "    test[col+\"_FREQ\"] = test[col].map(freq_dict)\n",
    "    \n",
    "#     agg_opts = ['mean']\n",
    "#     agg_dict = train.groupby(col).agg({'PSGR_COUNT':agg_opts}).to_dict()\n",
    "#     for opt in agg_opts:\n",
    "#         train[col+\"_\"+opt] = train[col].map(agg_dict[('PSGR_COUNT', opt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARRIER\n",
      "FLIGHT_NO\n",
      "AIRCRAFT_TYPE\n",
      "OND\n",
      "ORIGIN\n",
      "HUB\n",
      "DESTINATION\n"
     ]
    }
   ],
   "source": [
    "# for col in ['CARRIER', 'FLIGHT_NO', 'AIRCRAFT_TYPE', 'OND', 'ORIGIN','HUB', 'DESTINATION']:\n",
    "#     print(col)\n",
    "#     temp = train[[col,col+'_mean']].drop_duplicates()\n",
    "#     test = test.merge(temp,'left',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train2.pkl')\n",
    "# test.to_pickle('test2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parallelize_dataframe(df = train, func = day_features)\n",
    "test = parallelize_dataframe(df = test, func = day_features)\n",
    "# train.to_pickle('train3.pkl')\n",
    "# test.to_pickle('test3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pickle.load( open( \"train3.pkl\", \"rb\" ) )\n",
    "# test = pickle.load( open( \"test3.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"OND_ALL\"] = (train[\"ORIGIN\"].astype(\"str\")+train[\"HUB\"].astype(str)+train[\"DESTINATION\"].astype(str)).astype(\"category\")\n",
    "test[\"OND_ALL\"] = (test[\"ORIGIN\"].astype(\"str\")+test[\"HUB\"].astype(str)+test[\"DESTINATION\"].astype(str)).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"LEG1_DURATION\"] = ((train[\"LEG1_ARR_DATE_GMT\"] - train[\"LEG1_DEP_DATE_GMT\"])/np.timedelta64(60, 's')).astype(int)\n",
    "train[\"LEG2_DURATION\"] = ((train[\"LEG2_ARR_DATE_GMT\"] - train[\"LEG2_DEP_DATE_GMT\"])/np.timedelta64(60, 's')).astype(int)\n",
    "train[\"LEG_DURATION\"] = ((train[\"LEG2_ARR_DATE_GMT\"] - train[\"LEG1_DEP_DATE_GMT\"])/np.timedelta64(60, 's')).astype(int)\n",
    "\n",
    "test[\"LEG1_DURATION\"] = ((test[\"LEG1_ARR_DATE_GMT\"] - test[\"LEG1_DEP_DATE_GMT\"])/np.timedelta64(60, 's')).astype(int)\n",
    "test[\"LEG2_DURATION\"] = ((test[\"LEG2_ARR_DATE_GMT\"] - test[\"LEG2_DEP_DATE_GMT\"])/np.timedelta64(60, 's')).astype(int)\n",
    "test[\"LEG_DURATION\"] = ((test[\"LEG2_ARR_DATE_GMT\"] - test[\"LEG1_DEP_DATE_GMT\"])/np.timedelta64(60, 's')).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"OND_ALL\"\n",
    "freq_dict = pd.concat([train[col],test[col]]).value_counts().to_dict()\n",
    "train[col+\"_FREQ\"] = train[col].map(freq_dict)\n",
    "test[col+\"_FREQ\"] = test[col].map(freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90562838, 55)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARRIER 0\n",
      "FLIGHT_NO 0\n",
      "AIRCRAFT_TYPE 0\n",
      "OND 39\n",
      "OND_SELL_CLASS 0\n",
      "LEG1_SELL_CLASS 0\n",
      "OND_CABIN_CLASS 0\n",
      "LEG1_CABIN_CLASS 0\n",
      "ORIGIN 0\n",
      "HUB 0\n",
      "DESTINATION 0\n",
      "OND_ALL 127\n"
     ]
    }
   ],
   "source": [
    "# for col in ['CARRIER', 'FLIGHT_NO', 'AIRCRAFT_TYPE', 'OND', 'OND_SELL_CLASS', 'LEG1_SELL_CLASS', 'OND_CABIN_CLASS', 'LEG1_CABIN_CLASS', 'ORIGIN', 'HUB', 'DESTINATION', 'OND_ALL']:\n",
    "#     print(col,test.loc[test[col].isin(train[col])==False,col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATS_EXCLUDED = [\"ID_REC\",\"LEG1_DEP_DATE_GMT\",\"LEG1_ARR_DATE_GMT\",\"LEG2_DEP_DATE_GMT\",\"LEG2_ARR_DATE_GMT\",\"LEG1_DEP_TIME_GMT\",\n",
    "                  \"LEG1_DEP_TIME_GMT\",\"LEG1_ARR_TIME_GMT\",\"LEG2_DEP_TIME_GMT\",\"LEG2_ARR_TIME_GMT\",\n",
    "                  'CARRIER', 'FLIGHT_NO', 'AIRCRAFT_TYPE', 'OND', 'ORIGIN','HUB', 'DESTINATION']\n",
    "for col in FEATS_EXCLUDED:\n",
    "    if col in train:\n",
    "        del train[col], test[col]\n",
    "        \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"CARRIER_FREQ\"] = train[\"CARRIER_FREQ\"].astype(int)\n",
    "train[\"CARRIER_mean\"] = train[\"CARRIER_mean\"].astype(float)\n",
    "\n",
    "train[\"FLIGHT_NO_FREQ\"] = train[\"FLIGHT_NO_FREQ\"].astype(int)\n",
    "train[\"FLIGHT_NO_mean\"] = train[\"FLIGHT_NO_mean\"].astype(float)\n",
    "\n",
    "train[\"AIRCRAFT_TYPE_FREQ\"] = train[\"AIRCRAFT_TYPE_FREQ\"].astype(int)\n",
    "train[\"AIRCRAFT_TYPE_mean\"] = train[\"AIRCRAFT_TYPE_mean\"].astype(float)\n",
    "\n",
    "test[\"CARRIER_FREQ\"] = test[\"CARRIER_FREQ\"].astype(int)\n",
    "test[\"CARRIER_mean\"] = test[\"CARRIER_mean\"].astype(float)\n",
    "\n",
    "test[\"FLIGHT_NO_FREQ\"] = test[\"FLIGHT_NO_FREQ\"].astype(int)\n",
    "test[\"FLIGHT_NO_mean\"] = test[\"FLIGHT_NO_mean\"].astype(float)\n",
    "\n",
    "test[\"AIRCRAFT_TYPE_FREQ\"] = test[\"AIRCRAFT_TYPE_FREQ\"].astype(int)\n",
    "test[\"AIRCRAFT_TYPE_mean\"] = test[\"AIRCRAFT_TYPE_mean\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (90562838, 39)\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttrain's rmse: 2.60389\tvalid's rmse: 2.63329\n",
      "[2000]\ttrain's rmse: 2.57422\tvalid's rmse: 2.60941\n",
      "[3000]\ttrain's rmse: 2.55596\tvalid's rmse: 2.59551\n",
      "[4000]\ttrain's rmse: 2.54272\tvalid's rmse: 2.58583\n",
      "[5000]\ttrain's rmse: 2.53129\tvalid's rmse: 2.57785\n",
      "[6000]\ttrain's rmse: 2.52148\tvalid's rmse: 2.57084\n",
      "[7000]\ttrain's rmse: 2.51352\tvalid's rmse: 2.56566\n",
      "[8000]\ttrain's rmse: 2.50573\tvalid's rmse: 2.5606\n",
      "[9000]\ttrain's rmse: 2.49892\tvalid's rmse: 2.55645\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9999]\ttrain's rmse: 2.49246\tvalid's rmse: 2.55233\n"
     ]
    }
   ],
   "source": [
    "params = {'objective': 'regression','metric': 'rmse','learning_rate': 0.05,'verbose': -1,'nthread':32,\n",
    "          'num_leaves': 10, 'min_data': 50, 'max_depth': 10, 'num_leaves': 31, 'min_data_in_leaf': 50, 'feature_fraction': 0.8,\n",
    "          'bagging_fraction': 0.8}\n",
    "\n",
    "models, sub_preds, oof_preds, feature_importance = kfold_lightgbm(params,train,test,FEATS_EXCLUDED,9999,LOCAL_TEST=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds1 = np.where(oof_preds<1,1,oof_preds)\n",
    "sub_preds1 = np.where(sub_preds<1,1,sub_preds)\n",
    "final_rmse = sqrt(mean_squared_error(train['PSGR_COUNT'], oof_preds1))\n",
    "final_mape = mape(train['PSGR_COUNT'], oof_preds1) \n",
    "final_mae = mean_absolute_error(train['PSGR_COUNT'], oof_preds1)\n",
    "print('Final rmse : %.6f -- mape : %.6f -- mae : %.6f' % (final_rmse,final_mape,final_mae)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = pd.read_csv(\"datathon_case_2/case_2_train_data.csv\",usecols=[\"ID_REC\"])\n",
    "test_id = pd.read_csv(\"datathon_case_2/case_2_result.csv\",usecols=[\"ID_REC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.DataFrame()\n",
    "train_pred[\"ID_REC\"] = train_id[\"ID_REC\"]\n",
    "train_pred[\"PSGR_COUNT\"] = oof_preds1\n",
    "train_pred.to_csv(\"model_target_oof_v1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.DataFrame()\n",
    "test_pred[\"ID_REC\"] = test_id[\"ID_REC\"]\n",
    "test_pred[\"PSGR_COUNT\"] = sub_preds1\n",
    "test_pred.to_csv(\"Result.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
